{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8kAV-sQ9DIEM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "dbcbfa6a-3b8e-4eb8-c0d9-44eca93f5497"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/preprocessed_data_csv.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-60378843e71d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/preprocessed_data_csv.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/preprocessed_data_csv.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/content/preprocessed_data_csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-sg5cplFqUN"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwjSNIIWFtrx"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZdU7VZhFya0"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LNpIsXvaXqz"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XL-c7ZgaeOf"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKaJTdCMatM6"
      },
      "outputs": [],
      "source": [
        "cancer_type_counts = df['cancer_type'].value_counts()\n",
        "print(cancer_type_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPWWb_dta3mV"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(20, 6))\n",
        "cancer_type_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title('Count of Cancer Types')\n",
        "plt.xlabel('Cancer Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHwnZWaOb_vq"
      },
      "outputs": [],
      "source": [
        "# Word cloud of content\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = ' '.join(df['content'].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Word Cloud of Content')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsqrblDad9Jf"
      },
      "outputs": [],
      "source": [
        "# Check for null values in 'content' column\n",
        "print(df['content'].isnull().sum())\n",
        "\n",
        "# Drop rows with null values in 'content' column\n",
        "df = df.dropna(subset=['content'])\n",
        "\n",
        "# Check datatype of 'content' column\n",
        "print(df['content'].dtype)\n",
        "\n",
        "# Convert 'content' to string datatype\n",
        "df['content'] = df['content'].astype(str)\n",
        "\n",
        "# Distribution of content length\n",
        "df['content_length'] = df['content'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['content_length'], bins=20, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Content Length')\n",
        "plt.xlabel('Content Length of the posts')\n",
        "plt.ylabel('Frequency of the posts')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP8PxOgpenu6"
      },
      "outputs": [],
      "source": [
        "# Date-wise distribution of posts\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['date'].dt.date.value_counts().sort_index().plot(kind='line', color='skyblue')\n",
        "plt.title('Date-wise Distribution of Posts')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "#By plotting the date-wise distribution of posts as a line graph, one can observe trends, spikes,\n",
        "#or patterns in posting activity over the period covered by the dataset. This information can be useful\n",
        "#for understanding user engagement, identifying popular posting times, or detecting anomalies in posting behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXui4E_3gWR7"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "df['sentiment'] = df['content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# Distribution of sentiment\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['sentiment'], bins=20, kde=True, color='green')\n",
        "plt.title('Distribution of Sentiment')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS35EDRAhw5K"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='cancer_type', y='sentiment', data=df, palette='viridis')\n",
        "plt.title('Sentiment Distribution by Cancer Type')\n",
        "plt.xlabel('Cancer Type')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAgEjvvYiepu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['month'] = df['date'].dt.to_period('M')\n",
        "monthly_sentiment = df.groupby('month')['sentiment'].mean()\n",
        "monthly_sentiment.plot(marker='o', color='skyblue')\n",
        "plt.title('Average Sentiment Over Time')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Sentiment')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzdmflZWix8k"
      },
      "outputs": [],
      "source": [
        "extreme_posts = df[(df['sentiment'] == df['sentiment'].max()) | (df['sentiment'] == df['sentiment'].min())]\n",
        "print(\"Most Positive Post:\")\n",
        "print(extreme_posts[extreme_posts['sentiment'] == extreme_posts['sentiment'].max()]['content'].iloc[0])\n",
        "print(\"\\nMost Negative Post:\")\n",
        "print(extreme_posts[extreme_posts['sentiment'] == extreme_posts['sentiment'].min()]['content'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhuSlWPUj1z3"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Example: Compare sentiment between two cancer types\n",
        "cancer_type1_sentiment = df[df['cancer_type'] == 'Cancer_Type_1']['sentiment']\n",
        "cancer_type2_sentiment = df[df['cancer_type'] == 'Cancer_Type_2']['sentiment']\n",
        "\n",
        "# Check for missing values\n",
        "if cancer_type1_sentiment.isnull().any() or cancer_type2_sentiment.isnull().any():\n",
        "    print(\"Error: Missing values detected in sentiment scores.\")\n",
        "else:\n",
        "    # Check group sizes\n",
        "    if len(cancer_type1_sentiment) < 2 or len(cancer_type2_sentiment) < 2:\n",
        "        print(\"Error: Insufficient data for t-test.\")\n",
        "    else:\n",
        "        # Calculate descriptive statistics\n",
        "        print(\"Descriptive Statistics for Cancer Type 1:\")\n",
        "        print(cancer_type1_sentiment.describe())\n",
        "        print(\"\\nDescriptive Statistics for Cancer Type 2:\")\n",
        "        print(cancer_type2_sentiment.describe())\n",
        "\n",
        "        # Plot histograms\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(cancer_type1_sentiment, bins=20, alpha=0.5, label='Cancer Type 1', color='blue')\n",
        "        plt.hist(cancer_type2_sentiment, bins=20, alpha=0.5, label='Cancer Type 2', color='orange')\n",
        "        plt.title('Histogram of Sentiment Scores by Cancer Type')\n",
        "        plt.xlabel('Sentiment Score')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Check variance\n",
        "        print(\"\\nVariance of Cancer Type 1:\", cancer_type1_sentiment.var())\n",
        "        print(\"Variance of Cancer Type 2:\", cancer_type2_sentiment.var())\n",
        "\n",
        "        # Perform t-test\n",
        "        t_stat, p_value = ttest_ind(cancer_type1_sentiment, cancer_type2_sentiment, equal_var=False)\n",
        "        print(\"\\nT-statistic:\", t_stat)\n",
        "        print(\"P-value:\", p_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ1BZH2BkI2N"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Logarithmic transformation of content length\n",
        "df['log_content_length'] = np.log10(df['content_length'] + 1)  # Adding 1 to avoid log(0)\n",
        "plt.figure(figsize=(10, 20))\n",
        "sns.scatterplot(x='log_content_length', y='sentiment', data=df, color='purple', alpha=0.5)\n",
        "plt.title('Content Length vs. Sentiment')\n",
        "plt.xlabel('Log(Content Length)')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk3i9sN6krJe"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation coefficient\n",
        "correlation_coefficient = df['log_content_length'].corr(df['sentiment'])\n",
        "\n",
        "# Print correlation coefficient\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
        "\n",
        "# Provide actionable insights\n",
        "if correlation_coefficient > 0:\n",
        "    print(\"There is a positive correlation between content length and sentiment.\")\n",
        "elif correlation_coefficient < 0:\n",
        "    print(\"There is a negative correlation between content length and sentiment.\")\n",
        "else:\n",
        "    print(\"There is no significant correlation between content length and sentiment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmxtFcaTnba6"
      },
      "outputs": [],
      "source": [
        "# 1. Identify Patterns or Trends\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='log_content_length', y='sentiment', data=df, color='skyblue', alpha=0.5)\n",
        "plt.title('Content Length vs. Sentiment')\n",
        "plt.xlabel('Log(Content Length)')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhfecsdbntHG"
      },
      "outputs": [],
      "source": [
        "# 2. Analyze Correlation\n",
        "correlation_coefficient = df['log_content_length'].corr(df['sentiment'])\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG1dNCx9n5Mn"
      },
      "outputs": [],
      "source": [
        "# 3. Segmentation Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "scatter = sns.scatterplot(x='log_content_length', y='sentiment', hue='cancer_type', data=df, palette='viridis', alpha=0.5)\n",
        "plt.title('Content Length vs. Sentiment (Segmented by Cancer Type)')\n",
        "plt.xlabel('Log(Content Length)')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.legend(title='Cancer Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping elements\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDhmmtv9n69y"
      },
      "outputs": [],
      "source": [
        "# 4. Outlier Detection\n",
        "outliers = df[(df['log_content_length'] > 2) & (df['sentiment'] > 0.5)]  # Example condition for outliers\n",
        "print(\"Number of Outliers:\", len(outliers))\n",
        "print(\"Outlier Examples:\")\n",
        "print(outliers[['log_content_length', 'sentiment']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd5KXQDGpTwp"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Topic Modeling\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "content_matrix = vectorizer.fit_transform(df['content'])\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_model.fit(content_matrix)\n",
        "\n",
        "# Print top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for i, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Top words for Topic {i+1}:\")\n",
        "    print([feature_names[index] for index in topic.argsort()[-10:]])\n",
        "\n",
        "\n",
        "#Topic 1: This topic seems to be related to experiences with surgery, pain, and medical professionals (e.g., doctors).\n",
        "\n",
        "#Topic 2: This topic appears to be associated with medical terms related to diagnosis and treatment, such as nodes,\n",
        "#biopsy, scan, tumor, and cancer.\n",
        "\n",
        "#Topic 3: This topic might be related to personal experiences with cancer diagnosis and treatment, including interactions with\n",
        "#doctors and emotional responses.\n",
        "\n",
        "#Topic 4: This topic seems to involve aspects of life as a cancer patient, including treatment, time, and the impact on life.\n",
        "\n",
        "#Topic 5: This topic appears to focus on specific treatment modalities like radiation and chemotherapy, as well as the experience of\n",
        "#being diagnosed with cancer.\n",
        "\n",
        "#Based on these interpretations, you can see that each topic represents a different theme or aspect related to cancer diagnosis,\n",
        "#treatment, and personal experiences. These topics can provide valuable insights into the underlying structure of the corpus and help\n",
        "#organize and summarize the content of the documents. Depending on your specific goals or application, you can use these topics for tasks\n",
        "#such as document categorization, content recommendation, or sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zvTtPOusA9r"
      },
      "outputs": [],
      "source": [
        "# Select the source and target columns to create edges_df\n",
        "edges_df = df[['username', 'comments']].copy()\n",
        "\n",
        "# Display the first few rows of edges_df\n",
        "print(edges_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQZI3Ntg191A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "df['cancer_type'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=sns.color_palette('pastel'), startangle=140)\n",
        "plt.title('Cancer Type Distribution')\n",
        "plt.ylabel('')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFxDNJsi4EBl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "pairplot = sns.pairplot(df[['content_length', 'sentiment']], diag_kind='kde', plot_kws={'color': 'skyblue'})\n",
        "pairplot.fig.suptitle('Pairplot of Content Length and Sentiment')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMfmhDIa9NRO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='cancer_type', y='sentiment', data=df, palette='pastel')\n",
        "plt.title('Average Sentiment by Cancer Type')\n",
        "plt.xlabel('Cancer Type')\n",
        "plt.ylabel('Average Sentiment')\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NemKj6tj-Myi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='cancer_type', y='sentiment', data=df, palette='pastel')\n",
        "plt.title('Violin Plot of Sentiment Distribution by Cancer Type')\n",
        "plt.xlabel('Cancer Type')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.xticks(rotation=85)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsVOh3e1-08q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group by 'username' and count the number of comments for each user\n",
        "user_comment_counts = df['username'].value_counts()\n",
        "\n",
        "# Get the top 10 users with the most comments\n",
        "top_10_users = user_comment_counts.head(10)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_users.plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 10 Users by Number of Comments')\n",
        "plt.xlabel('Usernames')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVbQeqER_Orm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combine all comments into a single string\n",
        "comments = ' '.join(df['content'])\n",
        "\n",
        "# Tokenize the comments\n",
        "tokens = word_tokenize(comments)\n",
        "\n",
        "# Define English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stopwords and non-alphabetic tokens\n",
        "filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate frequency distribution of words\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Get the top 10 most frequent words\n",
        "top_words = freq_dist.most_common(10)\n",
        "\n",
        "# Plot the top 10 most frequent words\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(*zip(*top_words), color='skyblue')\n",
        "plt.title('Top 10 Most Frequent Words in Content (excluding stopwords)')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXZLz3-6_doF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'date' column already in datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Group by date and count the number of posts for each date\n",
        "post_count_by_date = df.resample('D', on='date').size()\n",
        "\n",
        "# Plot the number of posts over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "post_count_by_date.plot(linewidth=2, color='skyblue')\n",
        "plt.title('Number of Posts Over Time', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Number of Posts', fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewt2621UBH7N"
      },
      "outputs": [],
      "source": [
        "!pip install python-louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TytSFUPLBp_O"
      },
      "outputs": [],
      "source": [
        "!pip install networkx python-louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXCgIygeCr2d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'comments' column exists in your main DataFrame 'df'\n",
        "# Replace 'comments' with the appropriate column name if it differs in your dataset\n",
        "df_comments = df[['username', 'comments']].dropna()  # Assuming 'comments' is the column name for comments data\n",
        "\n",
        "# Rename columns to match the expected column names in the community network code\n",
        "df_comments.rename(columns={'username': 'Author', 'comments': 'Comment'}, inplace=True)\n",
        "\n",
        "# Display the first few rows of the newly created DataFrame\n",
        "print(df_comments.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from networkx.algorithms import community\n",
        "\n",
        "# Function to draw network graph with enhanced features\n",
        "def draw_network_graph(G, cancer_type):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Use spring layout with adjusted parameters for better node placement\n",
        "    pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
        "\n",
        "    # Check if the graph has edges\n",
        "    if G.number_of_edges() > 0:\n",
        "        # Detect communities within the network\n",
        "        communities = community.greedy_modularity_communities(G)\n",
        "        num_communities = len(communities)\n",
        "\n",
        "        # Draw nodes, coloring nodes by community\n",
        "        for i, community_nodes in enumerate(communities):\n",
        "            nx.draw_networkx_nodes(G, pos, nodelist=community_nodes, node_color=plt.cm.tab10(i), node_size=200, label=f'Community {i+1}')\n",
        "    else:\n",
        "        # Draw nodes without community coloring\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=200, label='Nodes')\n",
        "\n",
        "    # Draw edges\n",
        "    nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.7)\n",
        "\n",
        "    # Draw labels for nodes with adjusted font size and position\n",
        "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', verticalalignment='center', horizontalalignment='left')\n",
        "\n",
        "    plt.title(f'User Interaction Network for {cancer_type}')\n",
        "    plt.legend(loc='upper left', fontsize='medium', bbox_to_anchor=(1, 1))  # Place legend outside plot area\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "    plt.show()\n",
        "\n",
        "# Iterate over unique cancer types\n",
        "for cancer_type in df['cancer_type'].unique():\n",
        "    # Filter dataframe for the current cancer type\n",
        "    cancer_df = df[df['cancer_type'] == cancer_type]\n",
        "\n",
        "    # Create a graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes for each unique username\n",
        "    unique_users = cancer_df['username'].unique()\n",
        "    G.add_nodes_from(unique_users)\n",
        "\n",
        "    # Add edges between users based on interactions\n",
        "    for index, row in cancer_df.iterrows():\n",
        "        # Define interactions (e.g., commenting on the same post)\n",
        "        interactions = [(user1, user2) for user1 in unique_users for user2 in unique_users\n",
        "                        if user1 != user2 and user1 in row['comments'] and user2 in row['comments']]\n",
        "        G.add_edges_from(interactions)\n",
        "\n",
        "    # Draw the network graph with enhanced features\n",
        "    draw_network_graph(G, cancer_type)\n"
      ],
      "metadata": {
        "id": "K_uTKDXPFfk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Filter Data for a Specific Cancer Type\n",
        "cancer_type = 'Breast Cancer'  # We can change this to the cancer type you are interested in\n",
        "filtered_data = df[df['cancer_type'] == cancer_type]\n",
        "\n",
        "# Step 2: Text Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the 'content' column\n",
        "filtered_data['processed_content'] = filtered_data['content'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Frequency Analysis\n",
        "# Flatten the list of processed tokens\n",
        "all_tokens = [token for sublist in filtered_data['processed_content'] for token in sublist]\n",
        "\n",
        "# Count the frequency of each token\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "# Get the most common symptoms (e.g., top 10)\n",
        "top_symptoms = word_freq.most_common(10)\n",
        "print(\"Top 10 Common Symptoms for\", cancer_type, \":\\n\", top_symptoms)\n",
        "\n",
        "# Step 4: Visualization\n",
        "# Create a word cloud for visualization\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_symptoms))\n",
        "\n",
        "# Plot the word cloud\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.title('Top 10 Common Symptoms for ' + cancer_type)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "5qL5D16wFim3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "8u8ly-nLsxN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Define Relevant Keywords for Side Effects\n",
        "side_effect_keywords = [\"nausea\", \"fatigue\", \"hair loss\", \"vomiting\", \"pain\", \"weight loss\"]  # Add more as needed\n",
        "\n",
        "# Step 2: Text Processing\n",
        "# Function to preprocess text (tokenization and lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text and convert to lowercase\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the 'comments' and 'content' columns\n",
        "df['processed_comments'] = df['comments'].apply(preprocess_text)\n",
        "df['processed_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Keyword Matching\n",
        "# Function to count occurrences of side effect keywords\n",
        "def count_side_effects(text_tokens):\n",
        "    return Counter([token for token in text_tokens if token in side_effect_keywords])\n",
        "\n",
        "# Count side effect occurrences in comments and content\n",
        "df['side_effects_comments'] = df['processed_comments'].apply(count_side_effects)\n",
        "df['side_effects_content'] = df['processed_content'].apply(count_side_effects)\n",
        "\n",
        "# Step 4: Count Occurrences of Each Side Effect\n",
        "# Combine counts from comments and content\n",
        "combined_side_effects = df['side_effects_comments'] + df['side_effects_content']\n",
        "\n",
        "# Calculate total occurrences of each side effect\n",
        "total_side_effect_counts = combined_side_effects.sum()\n",
        "\n",
        "# Print the result\n",
        "for side_effect, count in total_side_effect_counts.items():\n",
        "    print(f\"{side_effect}: {count} mentions\")\n"
      ],
      "metadata": {
        "id": "w1ShrzUMs3L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Define Relevant Keywords for Side Effects\n",
        "side_effect_keywords = [\n",
        "    \"nausea\", \"vomiting\", \"diarrhea\", \"constipation\", \"abdominal pain\", \"stomach pain\",\n",
        "    \"loss of appetite\", \"weight loss\", \"fatigue\", \"weakness\", \"dizziness\", \"headache\",\n",
        "    \"numbness\", \"tingling\", \"muscle pain\", \"joint pain\", \"back pain\", \"chest pain\",\n",
        "    \"shortness of breath\", \"coughing\", \"high blood pressure\", \"low blood pressure\",\n",
        "    \"irregular heartbeat\", \"palpitations\", \"fever\", \"chills\", \"night sweats\", \"infection\",\n",
        "    \"sore throat\", \"difficulty swallowing\", \"mouth sores\", \"hair loss\", \"skin rash\",\n",
        "    \"itching\", \"bruising\", \"bleeding\", \"anemia\", \"memory problems\", \"confusion\",\n",
        "    \"anxiety\", \"depression\", \"hallucinations\", \"seizures\", \"tremors\", \"vision changes\",\n",
        "    \"hearing changes\", \"difficulty walking\", \"loss of balance\"\n",
        "]\n",
        "\n",
        "\n",
        "# Step 2: Text Processing\n",
        "# Function to preprocess text (tokenization and lowercase)\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text and convert to lowercase\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the 'comments' and 'content' columns\n",
        "df['processed_comments'] = df['comments'].apply(preprocess_text)\n",
        "df['processed_content'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Step 3: Keyword Matching\n",
        "# Function to count occurrences of side effect keywords\n",
        "def count_side_effects(text_tokens):\n",
        "    return Counter([token for token in text_tokens if token in side_effect_keywords])\n",
        "\n",
        "# Define a function to apply to each group\n",
        "def get_side_effect_counts(group):\n",
        "    # Count side effect occurrences in comments and content for the current group\n",
        "    group['side_effects_comments'] = group['processed_comments'].apply(count_side_effects)\n",
        "    group['side_effects_content'] = group['processed_content'].apply(count_side_effects)\n",
        "\n",
        "    # Combine counts from comments and content\n",
        "    combined_side_effects = group['side_effects_comments'] + group['side_effects_content']\n",
        "\n",
        "    # Calculate total occurrences of each side effect for the current group\n",
        "    total_side_effect_counts = combined_side_effects.sum()\n",
        "\n",
        "    return total_side_effect_counts\n",
        "\n",
        "# Apply the function to each group (cancer type)\n",
        "side_effect_counts_by_cancer_type = df.groupby('cancer_type').apply(get_side_effect_counts)\n",
        "\n",
        "# Print the results\n",
        "for cancer_type, side_effect_counts in side_effect_counts_by_cancer_type.items():\n",
        "    print(f\"Side effects for {cancer_type}:\")\n",
        "    for side_effect, count in side_effect_counts.items():\n",
        "        print(f\"{side_effect}: {count} mentions\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "Jz8uigvWtaJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_type_counts = df['cancer_type'].value_counts()\n",
        "print(cancer_type_counts)"
      ],
      "metadata": {
        "id": "kFaf-oh1dwwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "# Extract unique cancer types\n",
        "cancer_types = df['cancer_type'].unique()\n",
        "\n",
        "# Find the maximum number of elements for combinations (all elements)\n",
        "max_set_size = len(cancer_types)\n",
        "\n",
        "# Prompt the user to input the size of the combination set\n",
        "set_size = int(input(\"Enter the size of the combination set (1 to \" + str(max_set_size) + \"): \"))\n",
        "\n",
        "# Ensure the input is within a valid range\n",
        "if set_size < 1 or set_size > max_set_size:\n",
        "    print(\"Invalid set size. Please enter a value between 1 and\", max_set_size)\n",
        "else:\n",
        "\n",
        "    # Get combinations\n",
        "    print(f\"\\nCombinations for set size {set_size}:\")\n",
        "    for combo in combinations(cancer_types, set_size):\n",
        "        print(combo)\n"
      ],
      "metadata": {
        "id": "8u_ApktngTfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "# Extract unique cancer types\n",
        "cancer_types = df['cancer_type'].unique()\n",
        "\n",
        "# Find the maximum number of elements for combinations (all elements)\n",
        "max_set_size = len(cancer_types)\n",
        "\n",
        "# Prompt the user to input the size of the combination set\n",
        "set_size = int(input(\"Enter the size of the combination set (1 to \" + str(max_set_size) + \"): \"))\n",
        "\n",
        "# Ensure the input is within a valid range\n",
        "if set_size < 1 or set_size > max_set_size:\n",
        "    print(\"Invalid set size. Please enter a value between 1 and\", max_set_size)\n",
        "else:\n",
        "\n",
        "    # Get combinations\n",
        "    combinations_list = list(combinations(cancer_types, set_size))  # Store combinations in a list\n",
        "    combination_count = len(combinations_list)  # Count the generated combinations\n",
        "\n",
        "    print(f\"\\nCombinations for set size {set_size} (Total Number of Combinations: {combination_count}):\")\n",
        "    for combo in combinations_list:\n",
        "        print(combo)\n"
      ],
      "metadata": {
        "id": "gjQvbcjqjkmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "# Assuming cancer types are in the 'cancer_type' column of your DataFrame 'df'\n",
        "# Assuming 'side_effect_counts_by_cancer_type' holds side effect counts for each type (from previous code)\n",
        "\n",
        "# Extract unique cancer types\n",
        "cancer_types = df['cancer_type'].unique()\n",
        "\n",
        "# Function to find commonly mentioned side effects within a set of cancer types\n",
        "def find_common_symptoms(type_set):\n",
        "    # Extract side effect counts for types in the set\n",
        "    type_side_effects = {cancer_type: side_effect_counts_by_cancer_type.loc[cancer_type] for cancer_type in type_set}\n",
        "\n",
        "    # Convert Counters to DataFrames and sum values\n",
        "    type_side_effects_df = pd.DataFrame(type_side_effects.values())\n",
        "    min_count = type_side_effects_df.sum(axis=0)\n",
        "\n",
        "    common_symptoms = [side_effect for side_effect, count in min_count.items() if count > 0]\n",
        "    return common_symptoms\n",
        "\n",
        "# Get user input for set size\n",
        "max_set_size = len(cancer_types)\n",
        "set_size = int(input(\"Enter the size of the combination set (1 to \" + str(max_set_size) + \"): \"))\n",
        "\n",
        "if set_size < 1 or set_size > max_set_size:\n",
        "    print(\"Invalid set size. Please enter a value between 1 and\", max_set_size)\n",
        "else:\n",
        "    # Generate combinations and count them\n",
        "    combinations_list = list(combinations(cancer_types, set_size))\n",
        "    combination_count = len(combinations_list)\n",
        "\n",
        "    print(f\"\\nCombinations for set size {set_size} (Total Number of Combinations: {combination_count}):\")\n",
        "\n",
        "    # Find common symptoms for each combination\n",
        "    for combo in combinations_list:\n",
        "        common_symptoms = find_common_symptoms(combo)\n",
        "        print(f\"Common symptoms for {combo}: {common_symptoms}\")\n"
      ],
      "metadata": {
        "id": "pf1xrWBhkGNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "# Assuming cancer types are in the 'cancer_type' column of your DataFrame 'df'\n",
        "# Assuming 'side_effect_counts_by_cancer_type' holds side effect counts for each type (from previous code)\n",
        "\n",
        "# Extract unique cancer types\n",
        "cancer_types = df['cancer_type'].unique()\n",
        "\n",
        "# Function to find commonly mentioned side effects within a set of cancer types\n",
        "def find_common_symptoms(type_set):\n",
        "    # Extract side effect counts for types in the set\n",
        "    type_side_effects = {cancer_type: side_effect_counts_by_cancer_type.loc[cancer_type] for cancer_type in type_set}\n",
        "\n",
        "    # Convert Counters to DataFrames and sum values\n",
        "    type_side_effects_df = pd.DataFrame(type_side_effects.values())\n",
        "    min_count = type_side_effects_df.sum(axis=0)\n",
        "\n",
        "    common_symptoms = [side_effect for side_effect, count in min_count.items() if count > 0]\n",
        "    return common_symptoms\n",
        "\n",
        "# Menu card for cancer type selection\n",
        "print(\"\\n**Menu Card for Cancer Type Selection**\")\n",
        "print(\"Select cancer types (enter corresponding numbers separated by spaces):\")\n",
        "for i, cancer_type in enumerate(cancer_types):\n",
        "    print(f\"{i+1}. {cancer_type}\")\n",
        "\n",
        "# User input for cancer type selection\n",
        "while True:\n",
        "    user_choice = input(\"\\nEnter cancer type numbers (separated by spaces) or 'q' to quit: \")\n",
        "    if user_choice.lower() == 'q':\n",
        "        break\n",
        "    try:\n",
        "        # Convert user input to a set of integers (handles duplicates)\n",
        "        selected_cancer_indices = set(int(num) - 1 for num in user_choice.split())  # Adjust for 0-based indexing\n",
        "\n",
        "        # Validate user input (ensure chosen indices are within range)\n",
        "        if not all(0 <= i < len(cancer_types) for i in selected_cancer_indices):\n",
        "            print(\"Invalid cancer type number(s). Please enter numbers between 1 and\", len(cancer_types))\n",
        "            continue\n",
        "\n",
        "        # Extract selected cancer types\n",
        "        selected_cancer_types = tuple([cancer_types[i] for i in selected_cancer_indices])\n",
        "\n",
        "        # Find common symptoms based on chosen cancer types\n",
        "        common_symptoms = find_common_symptoms(selected_cancer_types)\n",
        "        print(f\"\\nCommon symptoms for {selected_cancer_types}: {common_symptoms}\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter integers separated by spaces or 'q' to quit.\")\n"
      ],
      "metadata": {
        "id": "JBhsnp8RnJIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords  # Import stopwords\n",
        "\n",
        "\n",
        "def analyze_top_discussion_topics_per_type(df, n=10):\n",
        "    # Group discussions by cancer type\n",
        "    cancer_type_groups = df.groupby(\"cancer_type\")\n",
        "\n",
        "    # Analyze topics for each cancer type\n",
        "    for cancer_type, group_df in cancer_type_groups:\n",
        "        print(f\"\\nTop {n} Most Frequent Topics for {cancer_type} Discussions:\")\n",
        "\n",
        "        # Create a new column for combined text (replace with your actual column names)\n",
        "        group_df[\"text\"] = group_df[\"cancer_type\"] + \" \" + group_df[\"title\"] + \" \" + group_df[\"content\"] + \" \" + group_df[\"comments\"].str.join(\" \")\n",
        "\n",
        "        # Text preprocessing\n",
        "        stop_words = set(stopwords.words('english'))  # Create stop word set\n",
        "        group_df[\"text\"] = group_df[\"text\"].str.lower()  # Lowercase conversion\n",
        "        group_df[\"text\"] = group_df[\"text\"].str.replace(\"[^a-zA-Z0-9\\s]\", \"\")  # Remove punctuation\n",
        "        group_df[\"text\"] = group_df[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]) if isinstance(x, str) else x)  # Handles non-strings gracefully\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = group_df[\"text\"].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
        "\n",
        "        # Generate bigrams (2-word phrases)\n",
        "        all_bigrams = pd.Series(tokens.sum()).reset_index(name='word')  # Create DataFrame from word counts\n",
        "        #all_bigrams = all_bigrams.rename(columns={'word': 'count'})  # Optional: rename column\n",
        "\n",
        "        #print(all_bigrams.columns)\n",
        "        # Count occurrences (prioritize bigrams if trigrams are unavailable)\n",
        "        topic_counts = all_bigrams.groupby('word')['word'].sum().sort_values(ascending=False)  # Replace with actual column name\n",
        "        top_n_grams = topic_counts.head(n)\n",
        "\n",
        "        # Print top n most frequent topics\n",
        "        for topic, count in top_n_grams.items():\n",
        "            print(f\"- {topic}: {count} mentions\")\n",
        "\n",
        "\n",
        "# Load your cancer discussion dataset into a DataFrame named 'df' (replace with your actual loading steps)\n",
        "df = pd.read_csv(\"/content/preprocessed_data_csv.csv\")\n",
        "\n",
        "# Analyze top discussion topics for each cancer type\n",
        "analyze_top_discussion_topics_per_type(df.copy())\n"
      ],
      "metadata": {
        "id": "JCPDtp6_oQdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parse date strings into datetime objects (adjust if date format is different)\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Get minimum and maximum dates\n",
        "min_date = df['date'].min()\n",
        "max_date = df['date'].max()\n",
        "\n",
        "# Calculate the number of years\n",
        "num_years = (max_date.year - min_date.year)\n",
        "\n",
        "# Calculate the difference in days (consider hours too)\n",
        "time_delta = max_date - min_date\n",
        "\n",
        "# Print results\n",
        "print(\"Range of Dates:\")\n",
        "print(f\"- Oldest Date: {min_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"- Latest Date: {max_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"- Number of Years: {num_years}\")\n",
        "print(f\"- Number of Days: {time_delta.days}\")\n"
      ],
      "metadata": {
        "id": "Br4acJO16Uqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 2. Identifying Specific Emotions\n",
        "def identify_emotions(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        sentiment_score = sid.polarity_scores(text)\n",
        "        emotion = max(sentiment_score, key=sentiment_score.get)\n",
        "        return emotion\n",
        "    else:\n",
        "        return None  # Return None for NaN values\n",
        "\n",
        "# Apply the function to the 'content' column\n",
        "df['emotion'] = df['content'].apply(identify_emotions)\n"
      ],
      "metadata": {
        "id": "AJRAXRng9Ynl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "s35LwRDTyvOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# 3. Topic-Specific Sentiment\n",
        "def get_topics_sentiment(texts):\n",
        "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    X = vectorizer.fit_transform(texts.fillna(''))  # Fill NaN values with empty string\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X)\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    topic_sentiments = []\n",
        "    for text in texts:\n",
        "        X_text = vectorizer.transform([text])\n",
        "        topic_mixture = lda.transform(X_text)[0]\n",
        "        top_topics_idx = topic_mixture.argsort()[-5:]  # Get top 5 topics\n",
        "        topic_features = [feature_names[idx] for idx in top_topics_idx]\n",
        "        topic_sentiments.append(topic_features)\n",
        "\n",
        "    return topic_sentiments\n",
        "\n",
        "df['topics_sentiment'] = get_topics_sentiment(df['content'].fillna(''))  # Fill NaN values with empty string\n"
      ],
      "metadata": {
        "id": "ynIq8cAbyyR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Comparative Sentiment Analysis\n",
        "def compare_sentiment_across_groups(data, group_column, text_column):\n",
        "    sentiments_by_group = {}\n",
        "    groups = data[group_column].unique()\n",
        "\n",
        "    for group in groups:\n",
        "        group_data = data[data[group_column] == group]\n",
        "        sentiments = [sid.polarity_scores(text)['compound'] for text in group_data[text_column].fillna('')]  # Fill NaN values with empty string\n",
        "        sentiments_by_group[group] = sentiments\n",
        "\n",
        "    return sentiments_by_group\n",
        "\n",
        "sentiments_by_cancer_type = compare_sentiment_across_groups(df, 'cancer_type', 'content')"
      ],
      "metadata": {
        "id": "hiXuDpN71lGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the results\n",
        "print(\"Emotions Identified:\")\n",
        "print(df['emotion'].value_counts())\n",
        "\n",
        "print(\"\\nTopic-Specific Sentiment:\")\n",
        "for idx, features in enumerate(df['topics_sentiment']):\n",
        "    print(f\"Topic {idx + 1}: {', '.join(features)}\")\n",
        "\n",
        "print(\"\\nComparative Sentiment Analysis by Cancer Type:\")\n",
        "for cancer_type, sentiments in sentiments_by_cancer_type.items():\n",
        "    print(f\"{cancer_type}: Mean Sentiment {np.mean(sentiments):.2f}\")\n"
      ],
      "metadata": {
        "id": "B6-EV-N02o0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English NER model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to identify opinion targets using NER\n",
        "def identify_opinion_targets(text):\n",
        "    doc = nlp(text)\n",
        "    opinion_targets = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'PRODUCT']:  # Consider only organizations and products as opinion targets\n",
        "            opinion_targets.append(ent.text)\n",
        "    return opinion_targets\n",
        "\n",
        "# Function to perform frequency analysis of nouns\n",
        "def frequency_analysis_of_nouns(texts):\n",
        "    nouns = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        for token in doc:\n",
        "            if token.pos_ == 'NOUN':\n",
        "                nouns.append(token.text)\n",
        "    return Counter(nouns)\n",
        "\n",
        "# Example dataframe (replace this with your actual dataframe)\n",
        "data = {\n",
        "    'content': ['Some text', 'Some other text', float('nan'), 'More text']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Replace NaN values with empty strings\n",
        "df['content'] = df['content'].fillna('')\n",
        "\n",
        "df['opinion_targets_ner'] = df['content'].apply(identify_opinion_targets)\n",
        "noun_frequencies = frequency_analysis_of_nouns(df['content'])\n",
        "\n",
        "# Display the opinion targets identified using NER\n",
        "print(\"\\nOpinion Targets Identified using Named Entity Recognition (NER):\")\n",
        "for idx, targets in enumerate(df['opinion_targets_ner']):\n",
        "    if targets:\n",
        "        print(f\"Opinion Targets for entry {idx + 1}: {', '.join(targets)}\")\n",
        "\n",
        "# Display the most frequent nouns\n",
        "print(\"\\nMost Frequent Nouns:\")\n",
        "print(noun_frequencies.most_common(10))\n"
      ],
      "metadata": {
        "id": "a-Trum0E2vUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_and_emotions(text):\n",
        "  \"\"\"\n",
        "  This function analyzes sentiment and identifies emotions in a text string.\n",
        "\n",
        "  Args:\n",
        "      text: The text string to be analyzed.\n",
        "\n",
        "  Returns:\n",
        "      A dictionary containing sentiment polarity (positive, neutral, negative),\n",
        "      subjectivity score (0-1), and a list of detected emotions.\n",
        "  \"\"\"\n",
        "  blob = TextBlob(text)\n",
        "  sentiment = {\n",
        "      \"polarity\": blob.sentiment.polarity,  # Positive: > 0, Neutral: 0, Negative: < 0\n",
        "      \"subjectivity\": blob.sentiment.subjectivity  # 0: objective, 1: subjective\n",
        "  }\n",
        "  emotions = []\n",
        "  # Customize emotion detection based on your needs (consider VADER for more options)\n",
        "  if blob.sentiment.polarity > 0.5:\n",
        "    emotions.append(\"joy\")  # Add more positive emotions as needed\n",
        "  elif blob.sentiment.polarity < -0.5:\n",
        "    emotions.append(\"anger\")  # Add more negative emotions as needed\n",
        "  # Add logic to identify other emotions based on sentiment scores and word patterns\n",
        "\n",
        "  return {\"sentiment\": sentiment, \"emotions\": emotions}\n"
      ],
      "metadata": {
        "id": "vQijmpe73sIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column to store sentiment analysis results\n",
        "df['sentiment_analysis'] = df['content'].apply(get_sentiment_and_emotions)\n"
      ],
      "metadata": {
        "id": "S-rG_u1w41U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all emotions detected across reviews (handle potential empty lists)\n",
        "all_emotions = []\n",
        "for sentiment in df['sentiment_analysis']:\n",
        "  if sentiment:  # Check if sentiment is not empty\n",
        "    all_emotions.extend(sentiment['emotions'])\n",
        "\n",
        "# Count emotion occurrences (handle potential empty list)\n",
        "if all_emotions:  # Check if all_emotions has elements before creating Series\n",
        "  emotion_counts = pd.Series(all_emotions).value_counts()\n",
        "  print(\"Emotion Distribution:\")\n",
        "  print(emotion_counts)\n",
        "\n",
        "  # Visualize emotion distribution (optional)\n",
        "  emotion_counts.plot(kind='bar')\n",
        "  plt.title(\"Distribution of Emotions in Reviews\")\n",
        "  plt.show()\n",
        "else:\n",
        "  print(\"No emotions detected in the data.\")\n"
      ],
      "metadata": {
        "id": "zHW2sW4O44ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define topics/features of interest (replace with your actual topics)\n",
        "topics = [\"treatment\", \"side effects\", \"survival rate\"]\n",
        "\n",
        "# Analyze sentiment for each topic\n",
        "for topic in topics:\n",
        "  topic_reviews = df[df['content'].str.contains(topic, case=False)]  # Case-insensitive search\n",
        "  topic_sentiment = topic_reviews['sentiment_analysis'].apply(lambda x: x['sentiment']['polarity']).mean()\n",
        "  print(f\"Average sentiment for topic '{topic}': {topic_sentiment:.2f}\")\n"
      ],
      "metadata": {
        "id": "uEAFfKTy46dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/preprocessed_data_csv.csv')\n",
        "\n",
        "# Convert 'date' column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Drop rows with NaN values in 'content' column\n",
        "df = df.dropna(subset=['content'])\n",
        "\n",
        "# Initialize VADER for sentiment analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to perform sentiment analysis and return compound score\n",
        "def get_sentiment_score(text):\n",
        "    sentiment_score = sid.polarity_scores(text)\n",
        "    return sentiment_score['compound']\n",
        "\n",
        "# Apply sentiment analysis to each post\n",
        "df['sentiment_score'] = df['content'].apply(get_sentiment_score)\n",
        "\n",
        "# Extract year, month, season, and festival from the date\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['season'] = df['date'].dt.month % 12 // 3 + 1  # Calculate season based on month\n",
        "\n",
        "# Function to identify festival based on month\n",
        "def identify_festival(month):\n",
        "    if month == 1:\n",
        "        return 'New Year'\n",
        "    elif month == 5:\n",
        "        return 'Cinco de Mayo'\n",
        "    elif month == 7:\n",
        "        return 'Independence Day'\n",
        "    elif month == 10:\n",
        "        return 'Halloween'\n",
        "    elif month == 12:\n",
        "        return 'Christmas'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['festival'] = df['month'].apply(identify_festival)\n",
        "\n",
        "# Group by year, month, and festival, calculate average sentiment\n",
        "sentiment_by_time_and_festival = df.groupby(['year', 'month', 'season', 'festival']).agg({'sentiment_score': 'mean'}).reset_index()\n",
        "\n",
        "# Plot trend analysis by season and festival\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=sentiment_by_time_and_festival, x='season', y='sentiment_score', hue='festival')\n",
        "plt.title('Sentiment Analysis by Season and Festival')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Average Sentiment Score')\n",
        "plt.xticks(ticks=[0, 1, 2, 3], labels=['Spring', 'Summer', 'Autumn', 'Winter'])\n",
        "plt.legend(title='Festival')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KLnYGpiT5PNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Assuming your dataset is stored in a DataFrame called df\n",
        "# Further preprocessing may be required based on specific needs\n",
        "# For example, removing irrelevant columns, handling missing values, etc.\n",
        "\n",
        "# Step 2: Graph Creation\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes (users)\n",
        "G.add_nodes_from(df['username'], bipartite=0)\n",
        "G.add_nodes_from(df['title'], bipartite=1)\n",
        "\n",
        "# Add edges (interactions)\n",
        "for index, row in df.iterrows():\n",
        "    G.add_edge(row['username'], row['title'])\n",
        "\n",
        "# Step 3: Community Detection\n",
        "communities = nx.algorithms.community.greedy_modularity_communities(G)\n",
        "\n",
        "# Step 4: Influence Analysis\n",
        "# Calculate centrality measures\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
        "\n",
        "# Step 5: Social Media Graph Analytics\n",
        "# Calculate network metrics\n",
        "degree_distribution = nx.degree_histogram(G)\n",
        "clustering_coefficient = nx.average_clustering(G)\n",
        "average_shortest_path_length = nx.average_shortest_path_length(G)\n",
        "\n",
        "# Step 6: Visualization\n",
        "# Visualize the graph\n",
        "pos = nx.spring_layout(G)  # Positions for all nodes\n",
        "nx.draw(G, pos, with_labels=False, node_size=10)\n",
        "plt.show()\n",
        "\n",
        "# Visualize communities\n",
        "for i, community in enumerate(communities):\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=community, node_color=f\"C{i}\", node_size=10)\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qrO31FbQ7wD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPWxCUdc7CcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}